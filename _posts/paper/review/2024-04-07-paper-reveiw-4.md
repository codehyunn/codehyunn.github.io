---
title: "[ë…¼ë¬¸ ë¦¬ë·°] Attention is all you need"
header:
  #image: /assets/images/page-header-image.png
  #og_image: /assets/images/page-header-og-image.png
categories:
  - Paper-Review
tags:
  - Paper
  - Transformer
permalink: /paper-review/04/
toc: true
toc_sticky : true
last_modified_at: 2024-04-10
---

ì´ë²ˆì— ë¦¬ë·°í•  ë…¼ë¬¸ì€ Transfomerë¥¼ ë°œí‘œí•œ "Attention is all you need"ì´ë‹¤. ë…¼ë¬¸ ë§í¬ëŠ” [ì—¬ê¸°](https://arxiv.org/pdf/1706.03762.pdf) !

**ğŸ§šğŸ»â€â™€ï¸ì§§ê²Œ ì •ë¦¬í•´ë³´ìë©´ . . .**<br>
TransformerëŠ” RNNì´ë‚˜ CNN ì—†ì´ Attentionë§Œìœ¼ë¡œ êµ¬ì„±ëœ ìµœì´ˆì˜ ëª¨ë¸ì´ë‹¤.
{:.notice--blog-theme}


## | Introduction

### Recurrent Networks (RNN, LSTM, GRU)
- RNN ê³„ì—´ì˜ ëª¨ë¸ë“¤ì´ Sequence modeling, *Transductive learning ë¶„ì•¼ì—ì„œ SOTA ì„±ëŠ¥ì„ ë³´ì„.
- ì´ì „ ì‹œì ì˜ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒ ì‹œì ì˜ Hidden state vectorë¥¼ ê³„ì‚°í•˜ëŠ” RNN ëª¨ë¸ì˜ íŠ¹ì„± ìƒ ë³‘ë ¬ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•¨. 
- ê·¸ ê²°ê³¼, ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„  í•œê³„ì ì´ ì¡´ì¬í•¨.
  - í•´ê²°í•˜ê³ ì í•˜ëŠ” ë…¸ë ¥ì´ ë§ì•˜ê³ , ì„±ëŠ¥ì´ ì˜¤ë¥¸ ì‚¬ë¡€ë„ ìˆì—ˆì§€ë§Œ, ê·¼ë³¸ì ì¸ ë¬¸ì œ í•´ê²°ì€ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ.

### Attention mechanisms
- ì…ì¶œë ¥ ì‹œí€€ìŠ¤ ì‚¬ì´ì˜ ê±°ë¦¬ì— ìƒê´€ ì—†ì´ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë„ë¡ í•¨
- RNN ê³„ì—´ì˜ ëª¨ë¸ë“¤ê³¼ í•¨ê»˜ ì‚¬ìš©ë¨

### Transformer
- Attention mechanismë§Œìœ¼ë¡œ êµ¬í˜„ëœ ëª¨ë¸ì¸ **Transformer**ì„ ì œì•ˆ
- ê±°ë¦¬ì— ìƒê´€ì—†ì´ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë‘ ê³ ë ¤í•  ìˆ˜ ìˆìœ¼ë©° ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•¨

**ğŸ¨ Transductive learning**<br>
ê´€ì°°ëœ ë°ì´í„°(â–¶ï¸í•™ìŠµ ë°ì´í„°)ë¥¼ í†µí•´ íŠ¹ì • ë°ì´í„°(â–¶ï¸ê²€ì¦ ë°ì´í„°)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íƒœìŠ¤í¬ë¥¼ ì˜ë¯¸í•œë‹¤.
{:.notice}

## | Background
### Convolution Neural Network based Models
- ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ Convolution Neural Network(CNN)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , ë³‘ë ¬ì ìœ¼ë¡œ hidden representationì„ ê³„ì‚°í•˜ëŠ” ëª¨ë¸ì´ ë“±ì¥í•¨. 
  - ex. Extended Neural GPU, ByteNet, ConvS2S
- ì…ì¶œë ¥ ê°„ì˜ ê±°ë¦¬ì— ë”°ë¼ ê³„ì‚°ëŸ‰ì´ í¬ê²Œ ë‹¬ë¼ì§€ëŠ” ëª¨ìŠµì„ ë³´ì„.


### Self-attention & End-to-end memory network
- Self-attention (intra-attention)
  - ê° positionì˜ Representationì„ êµ¬í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ positionì˜ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ëŠ” Attention mechanism
- End-to-end memory netrwork
  - ë°˜ë³µì ì¸ Attention machanismì„ ê°€ì§€ëŠ” êµ¬ì¡°

### Transformer
- RNNì´ë‚˜ CNN ì—†ì´ "Only self-attention" ë§Œìœ¼ë¡œ ì…ì¶œë ¥ì˜ representationì„ ê³„ì‚°í•œ ìµœì´ˆì˜ ëª¨ë¸
- CNN ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬ ì—°ì‚°ëŸ‰ì´ ì¼ì •í•¨

## | Architecture
### Overall architecture
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/2e922830-248f-4733-a6ea-b91c833afce9){:.align-center}

- Neural sequence transduction modelê³¼ ë¹„ìŠ·í•œ êµ¬ì¡°ë¥¼ ê°€ì§
  - Input($$x_1, \cdots, x_n$$) â†’ ENCODER â†’ Representation($$z_1, \cdots, z_n$$) â†’ DECODER â†’ Output($$y_1, \cdots, y_n$$)
- Encoderì™€ Decoderê°€ stacked self-attention, point-wise fully connected layerë¡œ êµ¬ì„±ë¨

### Encoder and Decoder
- Encoderì™€ Decoder ëª¨ë‘ 6ê°œì˜ layerë¡œ ì´ë£¨ì–´ì§ (N=6)
- EncoderëŠ” 2ê°œì˜ sub-layer, DecoderëŠ” 3ê°œì˜ sub-layerë¡œ êµ¬ì„±ë˜ì–´ìˆìŒ
  1. Encoder
    - Multi-head self attention mechanism
    - Position-wise fully connected feed-forward network
  2. Decoder
    - Multi-head self attention mechanism<br>
      â†’ Decoderì—ì„œëŠ” ì´ì–´ì§€ëŠ” ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•˜ì—¬ ë³€í˜•ëœ(=Masked) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•¨
    - Position-wise fully connected feed-forward network
    - Multi-head self attention mechanism over Encoder outputs

- ëª¨ë“  sub-layerì—ëŠ” Residual connectionê³¼ Layer noramlizationì´ ì ìš©ë¨
  - Residual connectionì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•˜ì—¬, ëª¨ë“  ë ˆì´ì–´ì˜ Outputì€ ë™ì¼í•œ ì°¨ì›($$d_model$$,512)ì„ ê°€ì§

### Attention
#### Scaled Dot-Product Attention
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/cfd3883d-0beb-47e8-9fe6-ff858f3538f9){:.align-center}
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/7a744e63-ae8c-4c60-bb76-a78609e8f55a){:.align-center}

- Qì™€ KëŠ” $$d_k$$ì°¨ì›ì˜ queryì™€ key vectorë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ì´ê³ , VëŠ” $$d_v$$ì°¨ì›ì˜ value vectorë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ì„
- ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” attention functionì€ Additive attentionê³¼ Dot-product attentionì„
  1. Additive attention
    - í•˜ë‚˜ì˜ hidden layerë¥¼ ê°€ì§€ëŠ” Feed-forward networkë¡œ attentionì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•
  2. Dot-Product attention
    - Scaled Dot-Product attentionì—ì„œ scaling factorë¥¼ ì œì™¸í•œ ê²ƒê³¼ ê°™ìŒ
    - Additive attention ë³´ë‹¤ ë¹ ë¥´ê³  íš¨ìœ¨ì ì„
- $$\sqrt{d_k}$$ë¡œ Scalingí•˜ëŠ” ì´ìœ 
  - $$d_k$$ê°€ ì»¤ì§ˆìˆ˜ë¡ dot productì˜ ê·œëª¨ê°€ ì»¤ì§
    - $$q,k$$ì˜ mean=0, variance=1ì¼ ë•Œ, $$q\cdot k$$ì€ mean=0, variance=$$d_k$$ë¥¼ ê°€ì§.
  - ì´ì–´ì„œ Softmaxì˜ ë²”ìœ„ë„ ì»¤ì§€ê³ , Gradientë„ ë§¤ìš° ì‘ì•„ì§€ê²Œ ë¨
  - ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ Scaling ì§„í–‰.

#### Multi-Head Attention
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/154af730-21ae-4f5c-b67e-3997b00f44e8){:.align-center}
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/daddbc3e-994f-4e0b-81be-926ac6b3b1bd){:.align-center}

1. key, value, queryë¥¼ ê°ê° hë²ˆ ì„ í˜• ë³€í™˜
  - ì´ë•Œ ê°€ì¤‘ì¹˜(parameter matrix)ëŠ” ì„œë¡œ ë‹¤ë¦„
2. attention functionì„ ë³‘ë ¬ì ìœ¼ë¡œ ì ìš© 
3. hê°œì˜ ê²°ê³¼ë¥¼ concatí•˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ ì„ í˜• ë³€í™˜

#### Applications of Attention
1. Encoder-Decoder attention
  - QueryëŠ” previous decoder layerì—ì„œ, Keyì™€ ValueëŠ” encoder outputì—ì„œ ì˜´
  - Decoderì˜ ëª¨ë“  positionì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  positionë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŒ
2. Encoderì˜ Self-attention 
  - Query, Key, Value ëª¨ë‘ previous encoder layerì˜ outputì„
  - Encoderì˜ ëª¨ë“  positionì—ì„œ previous encoder layerì˜ ëª¨ë“  positionë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŒ
3. Decoderì˜ Self-attention
  - Query, Key, Value ëª¨ë‘ previous decoder layerì˜ outputì„
  - Decoderì˜ ëª¨ë“  positionì—ì„œ í˜„ì¬ positionê¹Œì§€ ì°¸ê³ í•  ìˆ˜ ìˆìŒ
  - *auto-regressive propertyë¥¼ ì§€í‚¤ê¸° ìœ„í•˜ì—¬ Decoderì—ì„œ *leftward information flowë¥¼ ë§‰ìŒ
  - softmaxë¥¼ ì ìš©í•˜ê¸° ì „ì— ë¯¸ë˜ ì‹œì ì— í•´ë‹¹í•˜ëŠ” ê°’ë“¤ì„ $$-\inf$$ë¡œ ë§ˆìŠ¤í‚¹í•¨

**ğŸ¨ Auto-regressive & Leftward information flow**<br>
â–¶ï¸Auto-regressive : ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ í˜„ì¬ ê°’ì´ ì´ì „ì˜ ê°’ë“¤ì— ì˜ì¡´í•˜ëŠ” íŠ¹ì„±<br>
â–¶ï¸Leftward information flow : í˜„ì¬ ì‹œì ë³´ë‹¤ ë¯¸ë˜ ì‹œì ì˜ ì •ë³´ë¥¼ ë³´ê³  ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ì˜ë¯¸
{:.notice}

### Position-wise Feed-Forward Networks
- ëª¨ë“  Attention sub-layer ë‹¤ìŒì—ëŠ” Fully connected feed-forward network(FFN)ê°€ ìˆìŒ
- ëª¨ë“  FFNì˜ êµ¬ì¡°ëŠ” ê°™ì§€ë§Œ, íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ë¦„
- ë‘ ë²ˆì˜ ì„ í˜• ë³€í˜•ê³¼ ê·¸ ì‚¬ì´ì˜ ReLU í•¨ìˆ˜ë¡œ ì´ë£¨ì–´ì¡Œìœ¼ë©° ì…ì¶œë ¥ ì°¨ì›ì€ $$d_{model}$$(512)ì„.
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/d96b79ce-2141-4cd4-91b7-065517150e8d){:.align-center}

### Embeddings and Softmax
- ì…ì¶œë ¥ì„ $$d_{model}$$(512)ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í˜•ì‹œí‚¤ê¸° ìœ„í•´ Learned embeddings ì‚¬ìš©
- ì¶œë ¥ì„ ë‹¤ìŒ í† í°ì˜ ì˜ˆì¸¡ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í˜•ì‹œí‚¤ê¸° ìœ„í•´ Learned linear transformation, softmax function ì‚¬ìš©
- ì—¬ê¸°ì„œ ì–¸ê¸‰ëœ Embedding layerì™€ linear transformationì€ ê°™ì€ weightë¥¼ ì‚¬ìš©í•¨

### Positional Encoding
- Attention êµ¬ì¡°ëŠ” ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì ˆëŒ€/ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ë”°ë¡œ ì£¼ì…í•¨
- ì…ë ¥ embeddingì— Positional encodingsë¥¼ ë”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, positional encodingë„ $$d_{model}$$(512)ì˜ ì°¨ì›ì„ ê°€ì§
- ë…¼ë¬¸ì—ì„œëŠ” sine, cosine í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ positional encodingì„ êµ¬í•¨
  ![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/e5d14917-be2f-49c6-9d08-d57718a3ca5c){:.align-center}
  - $$pos$$ëŠ” positionì„, $$i$$ëŠ” ì°¨ì›ì„ ì˜ë¯¸í•¨
  - ê° ì°¨ì›ì˜ positional encodingì€ sinusoidì˜ í˜•íƒœë¥¼ ë„ëŠ”ë°, ì°¨ì›ë§ˆë‹¤ ë‹¤ë¥¸ ì£¼ê¸°ë¥¼ ê°€ì§€ëŠ” í•¨ìˆ˜ì— ë§¤ì¹­ë¨
  - sine, cosine í•¨ìˆ˜ë¥¼ í†µí•´ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì„ í˜• í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒë³´ë‹¤ í•™ìŠµì— ë„ì›€ì´ ë  ê²ƒì´ë¼ê³  ì˜ˆìƒí•˜ì—¬ sine í•¨ìˆ˜ë¥¼ ì„ íƒí•¨
  - ì‹¤í—˜ ê²°ê³¼, Learned positional embeddingê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„. ê·¸ëŸ¬ë‚˜ sinusoid ë²„ì „ì€ ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆì—ˆê¸° ë•Œë¬¸ì— ìµœì¢…ì ìœ¼ë¡œ ì´ë¥¼ ì„ íƒí•¨

## | Conclusion
- TransformerëŠ” attentionë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ first sequence transduction ëª¨ë¸ì„
- multi-headed self-attentionì„ í†µí•´ encoder-decoder êµ¬ì¡°ì˜ recurrent ë ˆì´ì–´ë¥¼ ëŒ€ì²´í•¨
- ê¸°ê³„ ë²ˆì—­ ë¶„ì•¼ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì— ë¹„í•´ ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì„
