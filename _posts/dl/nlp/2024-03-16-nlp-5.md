---
title: "[NLP] Seq2Seq with Attention"
header:
  #image: /assets/images/page-header-image.png
  #og_image: /assets/images/page-header-og-image.png
categories:
  - NLP
tags:
  - Seq2Seq
  - Attention
permalink: /dl/nlp/5/
use_math : true
toc: true
toc_sticky : true
last_modified_at: 2024-03-16
---
**🧚🏻‍♀️참고**<br>
이 포스트는 [Boostcourse의 '자연어처리의 모든 것'](https://www.boostcourse.org/ai330)을 듣고 정리한 포스트입니다.
{:.notice--blog-theme}

## | Seq2Seq

### Seq2Seq Model

Sequence-to-Sequenct(Seq2Seq) model이 활용되는 대표적인 분야는 바로 기계 번역(maching translation)이다. Seq2Seq model은 단어 시퀀스를 입출력으로 하며 **인코더**와 **디코더**로 이루어져있다. 인코더는 입력을 읽어들이는 RNN이고, 디코더는 출력을 하는 RNN이다. 같은 RNN이지만 인코더와 디코더는 파라미터를 서로 공유하지 않는, 서로 다른 모델이다. 

Seq2Seq 모델은 매 시점마다 중요한 정보를 hidden state vector($${h}$$)에 축적한다. 하지만 $${h}$$의 사이즈는 정해져있기 때문에, 시퀀스가 길어질수록 정보의 변형 가능성이 높다. 이를 보완하기 위해 제안된 방법 중 하나가 바로 **Attention**이다.

## | Attention
### Attention
Attention은 decoder의 각 시점에서 중요한 정보에만 집중하겠다는 아이디어에서 시작한다. 

![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/3e082d66-deda-41c3-802f-1bb4186eb030){: .align-center}

위의 그림을 통해 Attention의 과정을 살펴보자. 

제일 먼저, 입력을 받은 인코더가 $${h^e_1, h^e_2, h^e_3, h^e_4}$$를 생성하고, 마지막 시점에서 계산된 $${h}$$를 디코더에 입력($${h^d_0}$$)으로 전달한다. $${h^d_0}$$를 전달받은 디코더는 start 토큰의 임베딩 벡터인 $${x_1}$$과 $${h^d_0}$$를 선형 변환하여 $${h^d_1}$$을 생성한다. 

이제부터가 중요하다. 우선, 디코더의 $$h^d$$와 인코더의 $$h^e$$ 사이의 내적을 계산하여 벡터 간의 유사도를 구한다. 이렇게 계산된 값들은 Attention score라고 부른다. 예시에서는 4개의 $$h^e$$가 존재하니까 각각 $$h^d$$와 내적하여 총 4개의 attention score를 얻게 된다. 다음으로 Attention score에 Softmax 함수를 취해 확률값으로 변경시킨다. 이때 확률값으로 이루어진 벡터를 attention vector라고 한다. 마지막으로, Attention vector와 $${h^e_1, h^e_2, h^e_3, h^e_4}$$를 element-wise로 곱하고 더하여 $${h^e}$$의 가중 평균을 구한다. 그 결과가 바로 Attention output이다. 예측값의 경우, Attention output과 디코더를 통해 계산된 $${h^d_1}$$을 output layer에 입력으로 전달하여 얻을 수 있다.

간단하게 그림에 나타내보면🐨!
![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/d9e05417-dc37-4f00-8262-da1ea42e0b6f){: .align-center}

### Attention Mechanisms
Attention은 유사도를 구하는 방식에 따라 방법을 구분해볼 수 있다. 

$$ \begin{align} score(h_t, \bar{h_s}) &= h_t^T \bar{h_s} \\
                                       &= h_t^T W_a \bar{h_s} \\
                                       &= v_a^T tanh (W_a[h_t;\bar{h_s}])
    \end{align} $$

이 수식에서 $$h_t$$는 디코더의 hidden state vector를, $$\bar{h_s}$$는 인코더의 hidden state vector를 의미한다.

(1)은 위에서 사용했던 방법으로, 두 벡터의 내적을 통해 유사도를 구한다. (2)는 (1)과 달리 두 벡터 사이에 학습 가능한 행렬인 $$W_a$$가 존재한다. 이 행렬을 내적의 가중치로 두어 유사도를 계산하며, 행렬은 역전파 중에 학습된다. 마지막으로 (3)은 (1), (2)과 달리 신경망을 사용하는 방법이다. 주어진 두 벡터를 concat하고 선형 변환을 거쳐 유사도를 구한다.

### Attention의 장점
- 기계 번역의 성능을 향상시켰다.
- Gradient Vanishing 문제를 해결하였다.
- Attention distribution을 분석함으로써 디코더가 어느 부분에 집중하는지 알 수 있다.