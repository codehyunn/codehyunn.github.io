---
title: "[NLP] Word Embedding"
header:
  #image: /assets/images/page-header-image.png
  #og_image: /assets/images/page-header-og-image.png
categories:
  - NLP
tags:
  - Word2Vec
  - GloVe
permalink: /dl/nlp/2/
use_math : true
toc: true
toc_sticky : true
last_modified_at: 2024-02-25
---
**🧚🏻‍♀️참고**<br>
이 포스트는 [Boostcourse의 '자연어처리의 모든 것'](https://www.boostcourse.org/ai330)을 듣고 정리한 포스트입니다.
{:.notice--blog-theme}

## | Word Embedding
단어를 특정한 차원의 벡터로 표현하는 방법이다. **의미 상의 유사도를 반영**하기 때문에 비슷한 의미를 가지는 단어끼리는 벡터 간의 거리가 가깝고, 다른 단어는 멀다는 특징을 가진다. 

## | Word2Vec
Word2Vec는 벡터로 표현된, 인접한 단어들을 학습시키는 알고리즘이다. 비슷한 맥락의 단어는 비슷한 의미를 지닐 것이라는 가정에서 시작하여, 주변 단어를 통해 특정 단어의 의미를 파악할 수 있다고 본다.

### 알고리즘
먼저 주어진 문서를 단어 단위로 Tokenize하여 Vocabulary를 만들고, 각 단어를 one-hot vector로 표현한다. 여기까지는 Bag-of-Words와 동일하다. 하지만 이 다음부터 차이가 발생한다. 주어진 문장 혹은 문서에서 Sliding window를 통해 학습 데이터가 될 단어쌍을 선정한다. 이때 window의 가운데에 위치하는 중심 단어가 학습할 단어가 되고, 다른 단어들은 ground-truth가 된다. 즉, 중심 단어와 앞뒤 단어가 쌍을 이루어 학습 데이터가 되는 것이다.

**🐨예시 (1)**<br>
주어진 문장 : 'I study nlp' $$\rightarrow $$ Vocabulary : {'I', 'study', 'nlp'}<br>
Sliding window의 크기가 3이라면 단어쌍은 다음과 같다. ((중심단어, 앞뒤 단어 중 하나)의 형태)<br>
1️⃣ ('I', 'study'),<br>
2️⃣ ('study', 'I'), ('study', 'nlp'),<br>
3️⃣ ('nlp', 'study')
{:.notice}

Word2Vec을 학습하는 모델은 2개의 레이어로 이루어진다.

![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/992afbf3-d824-4867-a229-72dac9247690){: .align-center}


여기에서 Hidden layer의 노드 수($$N$$)는 하이퍼 파라미터이며, word embedding의 좌표공간 차원의 수와 같다. (아. word embedding의 좌표공간 차원은 vocabulary 길이, 그러니까 one-hot vector의 길이와는 무관하다.) 위에서 만든 단어쌍은 (input($$x$$), ground-truth($$y$$))로 학습에 사용되며, $$W_1$$의 column과 $$W_2$$의 row은 각각 input과 gt를 나타낸다. 학습이 끝난 후에는 $$W_1$$와 $$W_2$$ 중 하나를 선택하여 embedding vector로 사용하는데, 보통은 $$W_1$$을 사용한다고 한다.

**🐨예시 (2)**<br>
'I study nlp'의 여러 단어쌍 중 ('study', 'nlp')로 예시를 들어보자.<br>
Input인 'study'는 [0,1,0]로 표현이 가능하고, $$W_1$$의 2번째 column이 'study' vector이다.<br>
GT인 'nlp'는 [0,0,1]로 표현이 가능하고, $$W_2$$의 3번째 row이 'nlp' vector이다.<br>
{:.notice}

### 특징
word vector를 벡터 공간에 찍었을때, 벡터 사이의 관계는 단어 사이의 관계를 나타낸다. 쉽게 말해 아래 사진의 man-woman, uncle-aunt처럼 같은 관계에 있는 단어들은 벡터들 또한 같은 관계를 가진다는 거다. 신기하다.

![image](https://github.com/codehyunn/codehyunn.github.io/assets/87523224/a6965d95-4951-4e30-adb7-301aec96c295){: .align-center}

Word2Vec이 활용되는 대표적인 태스크는 Analogy Reasoning이나 Intrusion Detection이며, NLP의 전반적인 분야에 대해 성능을 높여주기도 했다.

## | GloVe
GloVe는 Global Vectors for Word Representation의 약자로, Word2Vec과 같이 word embedding을 학습하는 알고리즘이다. 하지만 모든 단어쌍으로 학습을 진행하던 Word2Vec과 달리 **co-occurrence matrix**를 계산하고 학습을 진행하여 동일한 단어쌍에 대한 학습을 줄였다는 특징이 있다. 중복 계산을 줄여 학습이 빠르며, 적은 데이터로도 학습이 잘 이루어진다는 장점을 가진다.