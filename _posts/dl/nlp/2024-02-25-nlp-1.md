---
title: "[NLP] Bag-of-Words"
header:
  #image: /assets/images/page-header-image.png
  #og_image: /assets/images/page-header-og-image.png
categories:
  - NLP
tags:
  - Bag-of-Words
  - Naive Bayes
permalink: /dl/nlp/1/
use_math : true
toc: true
toc_sticky : true
last_modified_at: 2024-02-25
---
**🧚🏻‍♀️참고**<br>
이 포스트는 [Boostcourse의 '자연어처리의 모든 것'](https://www.boostcourse.org/ai330)을 듣고 정리한 포스트입니다.
{:.notice--blog-theme}

## | Bag-of-Words
Bag-of-Words는 NLP 분야에서 딥러닝 이전에 많이 사용되던 기법이다. 이름 그대로 문장에서의 단어들을 가방에 넣어 모은 것과 같은 방법이다. 어떻게 이루어지는지 살펴보자.

### 1. Vocabulary 만들기
Vocabulary는 주어진 문장들에 등장하는 모든 단어들을 모은 집합이다. 한 단어가 여러번 등장하더라도 Vocabulary에는 한번만 등장하도록 구성한다.

**🐨예시 (1)**<br>
주어진 문장 : 'I like pizza', 'She loves this pizza'<br>
Vocabulary는 **{'I', 'like', 'pizza', 'she', 'loves', 'this'}** 이다.
{:.notice}

### 2. Vocabulary의 단어들을 one-hot vector로 인코딩하기
Vocabulary는 범주형 변수(Categorical variable)이므로 모델 학습에 활용하기엔 부적합하다. 대신 단어들을 **one-hot vector**로 변경해준다. 이때, 단어 사이의 의미나 표현의 유사성과는 상관없이 모든 단어 간의 거리가 $$\sqrt2$$이고, 코사인 유사도는 0임을 기억해두자. 이 점이 바로 요즘 많이 사용되는 word embedding과 대비되는 특징이기 때문이다.<br>

**🐨예시 (2)**<br>
예시 (1)의 단어들을 one-hot vector로 만들어보자. <br>
I : **[1,0,0,0,0,0]**, like : **[0,1,0,0,0,0]**, pizza : **[0,0,1,0,0,0]**,<br>
she : **[0,0,0,1,0,0]**, loves : **[0,0,0,0,1,0]**, this : **[0,0,0,0,0,1]**<br>
{:.notice}

### 3. 문장을 one-hot vector의 합으로 표현하기
문장을 구성하는 단어들의 one-hot vector를 더하여 문장을 표현한다. <br>

**🐨예시 (3)**<br>
'She loves this pizza'를 one-hot vector의 합으로 표현해보자.<br>
she + loves + this + pizza = [0,0,0,1,0,0] + [0,0,0,0,1,0] + [0,0,0,0,0,1] + [0,0,1,0,0,0] = **[0,0,1,1,1,1]**
{:.notice}

## | Naive Bayes Classifier
NaiveBayes Classifier는 Bag-of-Words로 표현한 문서들로 분류 작업을 수행할 때 사용된 방법이다. 먼저 문서와 클래스에 대한 Bayes' Rule을 알아보려고 하는데, 그전에 조건부 확률(Conditional probability)부터 간단하게 쓱 보고가자.

### 조건부 확률
주어진 사건이 일어났을 때 다른 사건이 일어날 확률을 의미한다. 다른 어떤 사건을 A, 주어진 사건을 B라고 하면 다음과 같이 표기할 수 있다.

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

### Bayes' Rule
조건부 확률의 수식을 정리하면 아래 식을 얻을 수 있다. 

$$P(A\cap B) = P(A|B)P(B) = P(B|A)P(A)$$

그리고 이 식을 이용하여 조건부 확률의 수식을 바꿔보면, 

$$P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)} = \frac{P(A|B)P(B)}{P(B)}$$

여기에서 B가 일어나지 않았을 때의 A의 확률인 $${P(A)}$$를 사전 확률(Prior), B가 일어났을 때의 A의 확률인 $${P(A\mid{B})}$$는 사후 확률(Posterior)라고 하며 $${P(B\mid{A})}$$는 가능도(likelihood), $${P(B)}$$는 증거(Evidence)라고 한다.

### Bayes' Rule for Documents and Classes
Bayes' Rule을 어떤 문서 d와 클래스 c에 대해 적용하면 이렇게 표현할 수 있다. 이때, 문서 d를 구성하는 단어를 $$w$$라고 하자.

$$ \begin{align} C_{MAP} & = argmax_{c\in C}{P(c|d)}\\
                         & = argmax_{c\in C}{\frac{P(d|c)P(c)}{P(d)}} \\
                         & = argmax_{c\in C}{P(d|c)P(c)} \\
                         & = argmax_{c\in C}{P(w_1, w_2, \cdots, w_n | c)P(c)}
   \end{align} $$ 

(1)의 MAP는 *'Maximum A Posterior'*의 약자이다. 즉 $$C_{MAP}$$는 사후 확률이 최대값을 가지는 클래스인 정답에 가장 가까운 클래스를 의미한다. 베이즈 정리에 따라서 식을 변경하였는데, (2)에서 문서 d가 발생할 확률인 $$P(d)$$는 상수여서 무시하였다. (3)의 클래스 c에 대해 d가 등장할 확률 $${P(d\mid{c})}$$은 d를 구성하는 모든 단어인 w가 등장할 확률과 같다고 볼 수 있으므로 c에 대해 w가 등장할 확률의 곱으로 표현하였다.

**🐨예시 (4)**<br>
Test data의 Class를 예측해보자.<br>
{:.notice}

| Train/Test | Document(d) | Documnet(w) | Class |
| :--------: | :---------: | :---------: | :---: | 
| Train | 1 | Image reconition uses convolutional neural networks | CV |
| Train | 2 | Transformer can be used for image classification task | CV |
| Train | 3 | Language modeling uses transformer | NLP |
| Train | 4 | Document classification task is language task | NLP |
| Test | 5 | Classification task uses transformer | ?? | 
{:.notice}

클래스 별 발생 확률 :  $$ P(C_{CV}) = \frac{2}{4} = \frac{1}{2}, P(C_{NLP}) = \frac{2}{4} = \frac{1}{2} $$ <br><br>
Bag-of-Words를 사용하여 Test data의 단어들에 대한 조건부 확률($$P(w|c)$$)을 구하면, <br>
$$P(w_{classification}|c_{CV}) = \frac{1}{10}, P(w_{task}|c_{CV}) = \frac{1}{10}, P(w_{uses}|c_{CV}) = \frac{1}{10}, P(w_{transformer}|c_{CV}) = \frac{1}{10}$$ <br>
$$P(w_{classification}|c_{NLP}) = \frac{1}{13}, P(w_{task}|c_{NLP}) = \frac{2}{13}, P(w_{uses}|c_{NLP}) = \frac{1}{13}, P(w_{transformer}|c_{NLP}) = \frac{1}{13}$$ <br><br>
이제 위의 두 확률값들을 곱하여 최종 확률을 계산하고, 큰 값을 가지는 클래스로 분류하면 끝이다. (계산은 .. pass .. )<br>
$$P(c_CV|d_5) = P(C_{CV}) * P(w_{classification}|c_{CV}) * P(w_{task}|c_{CV}) * P(w_{uses}|c_{CV}) * P(w_{transformer}|c_{CV})$$
$$P(c_NLP|d_5) = P(C_{NLP}) * P(w_{classification}|c_{NLP}) * P(w_{task}|c_{NLP}) * P(w_{uses}|c_{NLP}) * P(w_{transformer}|c_{NLP})$$
{:.notice}
